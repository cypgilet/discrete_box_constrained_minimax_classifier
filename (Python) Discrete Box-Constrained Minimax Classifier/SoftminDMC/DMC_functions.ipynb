{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmin Discrete Minimax Classifer for Imbalanced Datasets and Prior Probability Shifts\n",
    "\n",
    "#### References for these algorithms:\n",
    "Article [1]: *Discrete Box-Constrained Minimax Classifier for Uncertain and Imbalanced Class Proportions.* Cyprien Gilet, Susana Barbosa, Lionel Fillatre, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2020.\n",
    "\n",
    "Article [2]: *Adjusting Decision Trees for Uncertain Class Proportions.* Cyprien Gilet, Marie Guyomard, Susana Barbosa, Lionel Fillatre, ECML/PKDD Workshop on Uncertainty in Machine Learning (WUML), 2020.\n",
    "\n",
    "Article [3]: *Softmin Discrete Minimax Classifier for Imbalanced Classes and Prior Probability Shifts.* Cyprien Gilet, Marie Guyomard, SÃ©bastien Destercke, Lionel Fillatre, available soon, 2023.\n",
    "\n",
    "The authors sincerely thank Marie Guyomard for her contributions and her precious help in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import xlrd\n",
    "import random\n",
    "import time\n",
    "import seaborn as sn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import bernoulli\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-conditional risks and global risk associated with any classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class-conditional risks as established in equation (5) on the paper [1]:\n",
    "\n",
    "$$\\hat{R}_k\\left( \\delta \\right)  := \\sum_{l\\in\\mathcal{\\hat{Y}}} L_{kl}  \\, \\hat{\\mathbb{P}}(\\delta({X_{i}}) = l \\mid Y_i = k).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_risk(YR, Yhat, K, L): \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    confmat : Matrix\n",
    "        Confusion matrix.\n",
    "    '''\n",
    "   \n",
    "    confmat = np.zeros((K, K))\n",
    "    R = np.zeros((1, K))\n",
    "    YR_liste= YR.values.tolist()\n",
    "    for k in range(0, K):\n",
    "        mk = YR_liste.count([k+1])\n",
    "        if mk > 0:\n",
    "            Ik = np.where(YR == (k + 1))\n",
    "            for l in range(0, K):\n",
    "                confmat[k,l] = sum(Yhat[Ik[0]]==l+1)/mk\n",
    "        R[0,k] = L[k, :].dot(confmat[k, :]) \n",
    "    \n",
    "    return R, confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the global risk as established in equation (4) on the paper [1]:\n",
    "\n",
    "$$\\hat{r}\\left(\\delta\\right)  =  \\sum_{k\\in\\mathcal{Y}} \\hat{\\pi}_k  \\hat{R}_k\\left( \\delta \\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_risk(R, pi):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    pi : Array of floats\n",
    "        Class proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "        Global risk.\n",
    "    '''\n",
    "    \n",
    "    r = (pi[0].dot(R[0]))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class proportions associated with the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class proportions as established in equation (1) on the paper [1]:\n",
    "\n",
    "$$\\hat{\\pi}_k = \\frac{1}{m} \\sum_{i\\in\\mathcal{I}} \\mathbb{1}_{\\{Y_i = k\\}},\\;\\forall  k \\in \\mathcal{Y}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi(K, Y):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    Y : DataFrame\n",
    "        Real labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : Array of floats\n",
    "        Class proportions.\n",
    "    '''\n",
    "    \n",
    "    pi = np.zeros((1,K))\n",
    "    Y = Y.values.tolist()\n",
    "    \n",
    "    for k in range(0, K):\n",
    "        pi[0,k] = Y.count([k+1])/np.shape(Y)[0]\n",
    "   \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Discrete Box-Constrained Minimax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_DMC(XTrain, YRTrain, K, L, discretization, param_kmeans, param_DT, N, eps, Box):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XTrain : DataFrame\n",
    "        Features of the training samples.\n",
    "    YRTrain : DataFrame\n",
    "        Real labels of the trainind samples.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    discretization : str\n",
    "        Type of discrimination : {'kmeans', 'DT'}.\n",
    "    param_kmeans : list of parameters for the k-means discretization.\n",
    "            {None, [nbFoldsfitT, nbT, optionPlot, T_max]}\n",
    "                nbFoldsfitT : int\n",
    "                    Number of iterations in the cross-validation procedure for determining the number of centroids.\n",
    "                nbT : int\n",
    "                    Number of centroids to be tested .\n",
    "                optionPlot : int {0,1}\n",
    "                    1 plots figure,   0: does not plot figure.\n",
    "                T_max : int\n",
    "                    Maximum number of centroids.\n",
    "    param_DT : List of parameters for Decision Trees discretization\n",
    "            {None, [depth, nbFolds_FB_DT, optionPlot]}\n",
    "                depth : List\n",
    "                    List of maximal depth to be tested.\n",
    "                nbFolds_FB_DT : int\n",
    "                    Number of folds for the Cross Validation of the force_brute_DT function.\n",
    "                optionPlot : int {0,1}\n",
    "                    1 plots figure,   0: does not plot figure.\n",
    "    N : int\n",
    "        Number of iterations for the projected subgradient algorithm.\n",
    "    eps : float\n",
    "        Maximum gap risk between training and validation sets.\n",
    "    T : int\n",
    "        Optimal number of centroids for the discretization process.\n",
    "    Box : Array\n",
    "       {'none', matrix} : Box-constraint on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStarDMC : Array of floats\n",
    "        Least favorable priors.\n",
    "    pHatDMC : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    discretization_model : Model Object\n",
    "        Discretization process (kmeans or Decision Tree Model)\n",
    "    V_iter : Array of floats\n",
    "        Value of the V function at each iteration in the projected subgradient algorithm.\n",
    "    T_opt_DMC : int\n",
    "        Number of discrete profiles.\n",
    "\n",
    "    '''\n",
    "\n",
    "    print(\"Fit DMC ...\")\n",
    "    \n",
    "    # DISCRETIZATION OF THE FEATURES\n",
    "    \n",
    "    if discretization==\"kmeans\" :\n",
    "        \n",
    "        nbFoldsfitT = param_kmeans[0]\n",
    "        nbT = param_kmeans[1]\n",
    "        optionPlot = param_kmeans[2]\n",
    "        T_max = param_kmeans[3]\n",
    "    \n",
    "        T_opt_DMC = fit_NBcentroids_T(XTrain, YRTrain, K, L, nbFoldsfitT, nbT, eps, optionPlot, T_max)\n",
    "        kmeansDMC = KMeans(n_clusters = int(T_opt_DMC))\n",
    "        kmeansDMC.fit(XTrain)\n",
    "        discretization_model = kmeansDMC\n",
    "        XDTrain = kmeansDMC.labels_ + 1\n",
    "\n",
    "    if discretization==\"DT\" :\n",
    "        \n",
    "        depth = param_DT[0]\n",
    "        nbFolds_FB_DT = param_DT[1]\n",
    "        optionPlot = param_DT[2]\n",
    "\n",
    "        # Fit the depth of the decision tree with respect to eps\n",
    "        depth_opt = DT_force_brute(XTrain, YRTrain, K, L, nbFolds_FB_DT, depth, eps)\n",
    "        \n",
    "        # Fit the decision tree with depth = depth_opt\n",
    "        modele_Discr = DecisionTreeClassifier(ccp_alpha=0.0, \n",
    "                                           class_weight=None, \n",
    "                                           criterion='gini', \n",
    "                                           max_depth= depth_opt, \n",
    "                                           splitter='best').fit(XTrain, YRTrain)\n",
    "        discretization_model = modele_Discr\n",
    "        \n",
    "        # Discretization of the features using the fitted decision tree\n",
    "        XDTrain = discretisation_DT(XTrain, modele_Discr)\n",
    "        T_opt_DMC = np.max(XDTrain)\n",
    "   \n",
    "\n",
    "    # PROJECTED SUBGRADIENT ALGORITHM FOR COMPUTING piStarDMC:\n",
    "    pHatDMC = compute_pHat(XDTrain, YRTrain, K, T_opt_DMC)\n",
    "    piStarDMC, rStarDMC, RStarDMC, V_iter, stockpi = compute_piStar(pHatDMC, YRTrain, K, L, T_opt_DMC, N, optionPlot, Box)\n",
    "\n",
    "    ### Uncomment for Allowing to smooth the values of piStar at the last iterations and to avoid some coordinates to\n",
    "    ### vanish when dealing with a large number of classes:\n",
    "    # piStarMean = piStarDMC.copy()\n",
    "    # for k in range(K):\n",
    "    #     piStarMean[0,k] = np.mean(stockpi[k,N-20:N])\n",
    "    # piStarDMC = piStarMean/np.sum(piStarMean)\n",
    "    \n",
    "    return piStarDMC, pHatDMC, discretization_model, V_iter, T_opt_DMC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used in fit_DMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the number of centroids when diescretizing the features using the Kmeans algorithm\n",
    "\n",
    "The function *fit_NBcentroids_T* allows to compute the optimal number of centroids for achieving an acceptable generalization error. To this aim, this function performs a $nbFolds$-cross-validation procedure on the training set to select the optimal number of centroids (see Flowchart in Fig.4 in the paper [1]). This function calls the function *EvalCentroidsT_fold_f* at each iteration of the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_NBcentroids_T(X, YR, K, L, nbFolds, nbT, eps, optionPlot, T_max):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        Features.\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    nbFolds : int\n",
    "        Number of iterations in the cross-validation procedure on the traing set.\n",
    "    nbT : int\n",
    "        Number of centroids to be tested .\n",
    "    eps : float\n",
    "        Maximum gap risk between training and validation sets.\n",
    "    optionPlot : int {0,1}\n",
    "        1: plot figure,   0: does not plot figure.\n",
    "    T_max : int\n",
    "        Largest number of centroids to be tested (T_max < number of training instances)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    T_opt : int\n",
    "        Optimal number of centroids.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print('-> Fit the number of centroids:')\n",
    "    kf = KFold(n_splits=nbFolds)\n",
    "    stockrTrain = np.zeros((nbFolds,nbT))\n",
    "    stockrTest = np.zeros((nbFolds,nbT))\n",
    "    stockT = np.zeros((nbFolds,nbT))\n",
    "    \n",
    "    kf = KFold(n_splits=nbFolds)\n",
    "    \n",
    "    checkProp = 0\n",
    "    while checkProp < nbFolds:\n",
    "        checkProp = 0\n",
    "        kf = KFold(n_splits=nbFolds, shuffle=True, random_state=True)\n",
    "        for train, test in kf.split(X):\n",
    "            XTrain, XTest, YRTrain, YRTest = X.loc[train,:], X.loc[test,:], YR.loc[train,:], YR.loc[test,:]\n",
    "            Xtrain = XTrain.reset_index()\n",
    "            Xtrain.drop(['index'], axis='columns', inplace=True)\n",
    "            Xtest = XTest.reset_index()\n",
    "            Xtest.drop(['index'], axis='columns', inplace=True)\n",
    "            YRtrain = YRTrain.reset_index()\n",
    "            YRtrain.drop(['index'], axis='columns', inplace=True)\n",
    "            YRtest = YRTest.reset_index()\n",
    "            YRtest.drop(['index'], axis='columns', inplace=True)\n",
    "            # Compute the proportions\n",
    "            piTest = compute_pi(K, YRtest)\n",
    "            if piTest[0,1] >= 0.00006:\n",
    "                checkProp = checkProp + 1\n",
    "    \n",
    "    f = 0\n",
    "    for train, test in kf.split(X):\n",
    "        print('---> Sub-cross-validation procedure: Processing fold f =', f)\n",
    "        XTrain, XTest, YRTrain, YRTest = X.loc[train,:], X.loc[test,:], YR.loc[train,:], YR.loc[test,:]\n",
    "        stockrTrain[f,:], stockrTest[f,:], stockT[f,:] = EvalCentroidsT_fold_f(XTrain,YRTrain,XTest,YRTest,K,L,nbT, T_max)\n",
    "        f = f + 1\n",
    "    \n",
    "    # select T_opt\n",
    "    rTrainAv = np.mean(stockrTrain, axis = 0)\n",
    "    rTestAv = np.mean(stockrTest, axis = 0)\n",
    "    stockTAv = np.mean(stockT, axis = 0)\n",
    "    \n",
    "    indT = np.where(np.abs(rTrainAv - rTestAv) <= eps)\n",
    "    if np.shape(indT)[1] > 0:\n",
    "        T_opt = stockTAv[np.argmin(rTrainAv[indT])]\n",
    "    else:\n",
    "        T_opt = K\n",
    "    \n",
    "    # plot figure risk for all T\n",
    "    if optionPlot == 1:\n",
    "        rTrainSTD = np.std(stockrTrain, axis = 0)\n",
    "        rTestSTD = np.std(stockrTest, axis = 0)\n",
    "\n",
    "        plt.plot(stockTAv, rTrainAv, color = 'blue', label='Training subsets')\n",
    "        plt.plot(stockTAv, (rTrainAv + rTrainSTD), color = 'blue', linestyle = \"--\")\n",
    "        plt.plot(stockTAv, (rTrainAv - rTrainSTD), color = 'blue', linestyle = \"--\")\n",
    "\n",
    "        plt.plot(stockTAv, rTestAv, color = 'orange', label='Validation sets')\n",
    "        plt.plot(stockTAv, (rTestAv + rTestSTD), color = 'orange', linestyle = \"--\")\n",
    "        plt.plot(stockTAv, (rTestAv - rTestSTD), color = 'orange', linestyle = \"--\")\n",
    "        plt.axvline(x = T_opt, linewidth=2, color='g')\n",
    "\n",
    "        font = {'weight':'normal','size':16}\n",
    "        plt.title('Computation of T_opt', fontdict=font)\n",
    "        plt.xlabel('Number of centroids $T$', fontdict=font)\n",
    "        plt.ylabel('Global risk $r$', fontdict=font)\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.grid(which='minor', axis='x', ls='-.')\n",
    "        plt.legend(loc=2, shadow=True)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    print('-> Optimal number of centroids T_opt (with respect to epsT) =', T_opt)\n",
    "    \n",
    "    return int(T_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function EvalCentroidsT_fold_f compute the global risk of error on both the training on validation set for $nbT$ different number of centroids (see Flowchart in Fig.4 in the paper [1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvalCentroidsT_fold_f(XTrain, YRTrain, XTest, YRTest, K, L, nbT, T_max):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XTrain : DataFrame\n",
    "        Features of the training sample.\n",
    "    YRTrain : DataFrame\n",
    "        Real labels of the training sample.\n",
    "    XTest : DataFrame\n",
    "        Features of the validation sample.\n",
    "    YRTest : DataFrame\n",
    "        Real labels of the validation sample.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Matrix\n",
    "        Loss Function.\n",
    "    nbT : int\n",
    "        Number of centroids to be tested.\n",
    "    T_max : int\n",
    "        Number maximal of centroids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rTrain : Array of floats\n",
    "        Global risks for the training sample for each number of centroids.\n",
    "    rTest : Array of floats\n",
    "        Global risk for the validation sample for each number of centroids.\n",
    "    stockT : Vector\n",
    "        Number of centroids that have been tested.\n",
    "\n",
    "    '''\n",
    "\n",
    "    m = np.shape(XTrain)[0]\n",
    "    \n",
    "    # Establish all the numbers of centroids T to be tested\n",
    "    if m > T_max:\n",
    "        Tmax = T_max\n",
    "    else:\n",
    "        Tmax = m\n",
    "    a = 1/(nbT-1) * math.log(Tmax/K)\n",
    "    b = math.log(K) - a\n",
    "    stockT = np.zeros((1,nbT))\n",
    "    for T in range(0, nbT):\n",
    "        stockT[0,T] = np.round(math.exp(a*(T+1) + b))\n",
    "    stockT[0,0] = K\n",
    "    stockT[0,nbT-1] = Tmax\n",
    "    \n",
    "    rTrain = np.zeros((1,nbT))\n",
    "    rTest = np.zeros((1,nbT))\n",
    "    interT = 0\n",
    "    \n",
    "    for TT in stockT[0]:    \n",
    "        T = int(TT)\n",
    "        print('------> Processing number of centroids T =', T)\n",
    "        kmeans = KMeans(n_clusters = T)\n",
    "        kmeans.fit(XTrain)\n",
    "        XDTrain = kmeans.labels_ + 1\n",
    "        XDTest = kmeans.predict(XTest) + 1\n",
    "        \n",
    "        piTrain = compute_pi(K,YRTrain)\n",
    "        pHat = compute_pHat(XDTrain, YRTrain, K, T)\n",
    "        YhatTrain = delta_Bayes_discret(XDTrain,pHat,piTrain,K,L)\n",
    "        RTrain, confmat = compute_conditional_risk(YRTrain, YhatTrain, K, L)\n",
    "        rTrain[0,interT] = compute_global_risk(RTrain, piTrain)\n",
    "        \n",
    "        YhatTest = delta_Bayes_discret(XDTest,pHat,piTrain,K,L)\n",
    "        RTest, confmat = compute_conditional_risk(YRTest, YhatTest, K, L)\n",
    "        piTest = compute_pi(K,YRTest)\n",
    "        rTest[0,interT] = compute_global_risk(RTest, piTest)\n",
    "        \n",
    "        interT = interT + 1\n",
    "        \n",
    "    return rTrain, rTest, stockT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions used if the discretization of the features is processed using decision trees\n",
    "The following functions refer to our paper [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *DT_force_brute* allows to compute the optimal depth of the decision tree for achieving an acceptable generalization error. To this aim, this function performs a $nbFolds$-cross-validation procedure on the training set to select the optimal depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_force_brute(X, YR, K, L, nbFolds, depth, eps):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        Features.\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Matrix\n",
    "        Loss function.\n",
    "    nbFolds : int\n",
    "        Number of iteration for the cross-validation on the training set.\n",
    "    depth : Array\n",
    "        List of depths to be tested.\n",
    "    eps : float\n",
    "        Maximum gap risk between training and validation sets.\n",
    "        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    depth_opt : int\n",
    "        Optimal maximal depth to use.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    kf = KFold(n_splits=nbFolds)\n",
    "    print(\"-> Search of the optimal depth:\")\n",
    "\n",
    "    stockrTrain = np.zeros((nbFolds,len(depth)))\n",
    "    stockrTest = np.zeros((nbFolds,len(depth)))\n",
    "\n",
    "    \n",
    "    f=0  \n",
    "    # For all folds of CV\n",
    "    for train, test in kf.split(X):\n",
    "        print('---> Sub-cross-validation procedure: Processing fold f =', f)\n",
    "        #print(\"processing fold f =\", f) \n",
    "        XTrain, XTest, YRTrain, YRTest = X.loc[train,:], X.loc[test,:], YR.loc[train,:], YR.loc[test,:]\n",
    "        Xtrain = XTrain.reset_index()\n",
    "        Xtrain.drop(['index'], axis='columns', inplace=True)\n",
    "        Xtest = XTest.reset_index()\n",
    "        Xtest.drop(['index'], axis='columns', inplace=True)\n",
    "        YRtrain = YRTrain.reset_index()\n",
    "        YRtrain.drop(['index'], axis='columns', inplace=True)\n",
    "        YRtest = YRTest.reset_index()\n",
    "        YRtest.drop(['index'], axis='columns', inplace=True)\n",
    "\n",
    "        piTrain = compute_pi(K, YRtrain)\n",
    "        piTest = compute_pi(K, YRtest)\n",
    "\n",
    "        i=0\n",
    "        # for all possible depth\n",
    "        for d in depth:\n",
    "            print('------> Processing depth =', d)\n",
    "            # Fit the model\n",
    "            model_DT = Decision_Tree_fit(Xtrain, YRtrain, d, None)\n",
    "            # Test on the training sample\n",
    "            YhatTrainDT = Decision_Tree_predict(Xtrain,model_DT)\n",
    "            RDT_Train, confmatTrainDT = compute_conditional_risk(YRtrain, YhatTrainDT, K, L)\n",
    "            stockrTrain[f,i] = compute_global_risk(RDT_Train, piTrain)\n",
    "            # Test on the validation sample\n",
    "            YhatTestDT = Decision_Tree_predict(Xtest,model_DT)\n",
    "            RDT_Test, confR_DT_Test = compute_conditional_risk(YRtest, YhatTestDT, K, L)\n",
    "            stockrTest[f,i] = compute_global_risk(RDT_Test, piTest)\n",
    "\n",
    "            i=i+1\n",
    "        \n",
    "        f=f+1\n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    # select depth_opt\n",
    "    rTrainAv = np.mean(stockrTrain, axis = 0)\n",
    "    rTestAv = np.mean(stockrTest, axis = 0)\n",
    "    \n",
    "    indT = np.where(np.abs(rTrainAv - rTestAv) <= eps)\n",
    "    if np.shape(indT)[1] > 0:\n",
    "        depth_opt = depth[np.argmin(rTrainAv[indT])]\n",
    "    else:\n",
    "        depth_opt = depth[0]\n",
    "        print('-> Warning: there is no depth achieving abs(rTrain - rTest) < eps')\n",
    "    \n",
    "        \n",
    "        \n",
    "    # plot figure risk for all T\n",
    "    if optionPlot == 1:\n",
    "        rTrainSTD = np.std(stockrTrain, axis = 0)\n",
    "        rTestSTD = np.std(stockrTest, axis = 0)\n",
    "\n",
    "        plt.plot(depth, rTrainAv, color = 'blue', label='Training subsets')\n",
    "        plt.plot(depth, (rTrainAv + rTrainSTD), color = 'blue', linestyle = \"--\")\n",
    "        plt.plot(depth, (rTrainAv - rTrainSTD), color = 'blue', linestyle = \"--\")\n",
    "\n",
    "        plt.plot(depth, rTestAv, color = 'orange', label='Validation sets')\n",
    "        plt.plot(depth, (rTestAv + rTestSTD), color = 'orange', linestyle = \"--\")\n",
    "        plt.plot(depth, (rTestAv - rTestSTD), color = 'orange', linestyle = \"--\")\n",
    "        plt.axvline(x = depth_opt, linewidth=2, color='g')\n",
    "\n",
    "        font = {'weight':'normal','size':16}\n",
    "        plt.title('Computation of the optimal depth', fontdict=font)\n",
    "        plt.xlabel('Depth of the decision tree', fontdict=font)\n",
    "        plt.ylabel('Global risk $r$', fontdict=font)\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.grid(which='minor', axis='x', ls='-.')\n",
    "        plt.legend(loc=2, shadow=True)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    print('-> Optimal Depth (with respect to epsT) =', depth_opt)\n",
    "\n",
    "    return depth_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the decision tree for a given depth (function called at each iteration of the cross-validation procedure in the function DT_force_brute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree_fit(XTrain, YRTrain, depth, classweight):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XTrain : DataFrame\n",
    "        Features of the training sample.\n",
    "    YRTrain : DataFrame\n",
    "        Real labels of the training sample.\n",
    "    depth : int\n",
    "        Depth of the decision tree.\n",
    "    classweight : str\n",
    "        Weights associated with classes\n",
    "        The âbalancedâ mode uses the values of y to automatically adjust \n",
    "        weights inversely proportional to class frequencies in the input data as \n",
    "        n_samples / (n_classes * np.bincount(y)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_DT : Decision Tree Classifier Model\n",
    "        The Decision Tree model fitted.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    model_DT = DecisionTreeClassifier(criterion = 'gini', \n",
    "                                      splitter = 'best', \n",
    "                                      max_depth = depth, \n",
    "                                      class_weight = classweight).fit(XTrain, YRTrain)\n",
    "    return model_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using the decision tree (function used for evaluating the generalization error of the decision tree in the function DT_force_brute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree_predict(XTest, model):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XTest : DataFrame\n",
    "        Features of the testing sample.\n",
    "    model : Decision Tree Classifier Model\n",
    "        The Decision Tree model used for the prediction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Yhat_DT : Vector\n",
    "        Labels estimated by the Decision Tree Classifier.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Yhat_DT = model.predict(XTest)\n",
    "    \n",
    "    return Yhat_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization of the features using the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretisation_DT(X, modele) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        Features.\n",
    "    modele : Decision Tree Classifier Model\n",
    "        Decidion Tree model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xdiscr : Vector\n",
    "        Discretised features.\n",
    "\n",
    "    '''\n",
    "    Xdiscr = DecisionTreeClassifier.apply(modele, X, check_input=True)\n",
    "    # Rename each leaf\n",
    "    l=1\n",
    "    for i in np.unique(Xdiscr):\n",
    "        for j in range(len(Xdiscr)) :\n",
    "            if Xdiscr[j]==i :\n",
    "                Xdiscr[j]=l\n",
    "        l+=1\n",
    "    return Xdiscr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for computing pHat\n",
    "\n",
    "pHat refers to the equation (6) in the paper [1]:\n",
    "\n",
    "$$\\hat{p}_{kt}:=\\frac{1}{m_k}\\sum_{i \\in \\mathcal{I}_k}\\mathbb{1}_\\left\\{X_i=x_t\\right\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pHat(XDTrain, YRTrain, K, T):  \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XDTrain : Vector\n",
    "        Features of the training set after discretization.\n",
    "    YRTrain : DataFrame\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "\n",
    "    '''\n",
    "\n",
    "    pHat = np.zeros((K, T))\n",
    "    YRTrain_list = YRTrain.values.tolist()\n",
    "    \n",
    "    for k in range(0, K):\n",
    "        Ik = np.where(YRTrain == (k + 1))[0]\n",
    "        mk = YRTrain_list.count([k+1])\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            pHat[k,t] = sum(XDTrain[Ik]==t+1)/mk\n",
    "    \n",
    "    return pHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Projected subgradient algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows to compute the least favorable priors $\\pi^{\\star}$ using the procedure presented in equation (24) and in Algorithm 1 in the paper [1]:\n",
    "\n",
    "$$\\pi^{(n+1)} = \\mathrm{P}_{\\mathbb{U}}\\left( \\pi^{(n)} + \\frac{\\gamma_n}{\\eta_{n}} \\, g^{(n)}  \\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_piStar(pHat, YRTrain, K, L, T, N, optionPlot, Box):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "    YRTrain : Dataframe\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "    N : int\n",
    "        Number of iterations in the projected subgradient algorithm.\n",
    "    optionPlot : int {0,1}\n",
    "        1 plots figure,   0: does not plot figure.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraints on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "        Least favorable priors.\n",
    "    rStar : float\n",
    "        Global risks.\n",
    "    RStar : Array of float\n",
    "        Conditional risks.\n",
    "    V_iter : Array\n",
    "        Values of the V function at each iteration.\n",
    "    stockpi : Array\n",
    "        Values of pi at each iteration.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print('-> Compute the least favorable priors piStar')\n",
    "    \n",
    "    # IF BOX-CONSTRAINT == NONE (PROJECTION ONTO THE SIMPLEX)\n",
    "    if isinstance(Box, str) == True :\n",
    "        pi = compute_pi(K,YRTrain)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "        \n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "        \n",
    "        for n in range(1, N+1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:,t])\n",
    "                    mu_k = mu_k + L[k,lbar] * pHat[k,t]\n",
    "                R[0,k] = mu_k\n",
    "                stockpi[k,n-1] = pi[0,k]\n",
    "            r = compute_global_risk(R, pi) \n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R \n",
    "            # Update pi for iteration n+1\n",
    "            gamma = 1/n\n",
    "            eta = np.maximum(float(1),np.linalg.norm(R))\n",
    "            w = pi + (gamma/eta)*R\n",
    "            pi = proj_simplex_Condat(K,w)\n",
    "            \n",
    "            \n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:,t])\n",
    "                mu_k = mu_k + L[k,lbar] * pHat[k,t]\n",
    "            R[0,k] = mu_k\n",
    "            stockpi[k,n-1] = pi[0,k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "            \n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "        \n",
    "        \n",
    "    # IF BOX-CONSTRAINT\n",
    "    if isinstance(Box, str) == False :\n",
    "        pi = compute_pi(K,YRTrain)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "        \n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "        \n",
    "        for n in range(1, N+1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:,t])\n",
    "                    mu_k = mu_k + L[k,lbar] * pHat[k,t]\n",
    "                R[0,k] = mu_k\n",
    "                stockpi[k,n-1] = pi[0,k]\n",
    "            r = compute_global_risk(R, pi) \n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R \n",
    "            # Update pi for iteration n+1\n",
    "            gamma = 1/n\n",
    "            eta = np.maximum(float(1),np.linalg.norm(R))\n",
    "            w = pi + (gamma/eta)*R\n",
    "            pi = proj_onto_U(w, Box, K)\n",
    "            \n",
    "            \n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:,t])\n",
    "                mu_k = mu_k + L[k,lbar] * pHat[k,t]\n",
    "            R[0,k] = mu_k\n",
    "            stockpi[k,n-1] = pi[0,k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "            \n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "              \n",
    "        \n",
    "    return piStar, rStar, RStar, V_iter, stockpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection onto the simplex:\n",
    "\n",
    "This function is inspired from the article:\n",
    "L.Condat, \"Fast projection onto the simplex and the $\\ell_1$ ball\", *Mathematical Programming*, vol.158, no.1, pp. 575-585, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_simplex_Condat(K, pi):  \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    pi : Array of floats\n",
    "        Vector to project onto the simplex.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piProj : List of floats\n",
    "        Priors projected onto the simplex.\n",
    "\n",
    "    '''\n",
    "   \n",
    "    linK = np.linspace(1, K, K)\n",
    "    piProj = np.maximum(pi - np.max(((np.cumsum(np.sort(pi)[::-1]) - 1) / (linK[:]))),0)\n",
    "    piProj = piProj / np.sum(piProj)\n",
    "    return piProj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection onto the box-constrained simplex:\n",
    "\n",
    "The function proj_onto_polyhedral_set is inspired from the article:\n",
    "K. E. Rutkowski, âClosed-form  expressions for projectors ontopolyhedral sets in hilbert spaces, âSIAM Journal on Optimization, vol. 27, pp. 1758â1771, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_onto_U(pi, Box, K) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    pi : Array of floats\n",
    "        Vector to project onto the box-constrained simplex..\n",
    "    Box : Matrix\n",
    "        {'none', matrix} : Box-constraint on the priors.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi_new : Array of floats\n",
    "            Priors projected onto the box-constrained simplex.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    check_U = 0\n",
    "    if pi.sum() ==1 :\n",
    "        for k in range(K) :\n",
    "            if (pi[0][k] >= Box[k,0]) & (pi[0][k] <= Box[k,1]) :\n",
    "                check_U = check_U + 1\n",
    "    \n",
    "    if check_U == K :\n",
    "        pi_new = pi\n",
    "\n",
    "      \n",
    "    if check_U < K :\n",
    "        pi_new = proj_onto_polyhedral_set(pi, Box, K)\n",
    "    \n",
    "    return pi_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def num2cell(a):\n",
    "    if type(a) is np.ndarray:\n",
    "        return [num2cell(x) for x in a]\n",
    "    else:\n",
    "        return a \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def proj_onto_polyhedral_set(pi, Box, K) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    pi : Array of floats\n",
    "        Vector to project onto the box-constrained simplex.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraint on the priors.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "            Priors projected onto the box-constrained simplex.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Verification of constraints\n",
    "    for i in range(K) :\n",
    "        for j in range(2) :\n",
    "            if Box[i,j] < 0 :\n",
    "                Box[i,j] = 0\n",
    "            if Box[i,j] > 1 :\n",
    "                Box[i,j] = 1\n",
    "\n",
    "    # Generate matrix G:\n",
    "    U = np.concatenate((np.eye(K), -np.eye(K), np.ones((1,K)), -np.ones((1,K))))            \n",
    "    eta = Box[:,1].tolist() + (-Box[:,0]).tolist() + [1] + [-1]\n",
    "\n",
    "    n = U.shape[0]\n",
    "    \n",
    "    G = np.zeros((n,n))\n",
    "    for i in range(n) :\n",
    "        for j in range(n) :\n",
    "            G[i,j] = np.vdot(U[i,:],U[j,:])\n",
    "    \n",
    "    \n",
    "    # Generate subsets of {1,...,n}:\n",
    "    M = (2**n)-1\n",
    "    I = num2cell(np.zeros((1,M)))\n",
    "    \n",
    "    i = 0\n",
    "    for l in range(n) :\n",
    "        T = list(combinations(list(range(n)), l+1))\n",
    "        for p in range(i,i+len(T)) :\n",
    "            I[0][p] = T[p-i]\n",
    "        i = i+len(T)\n",
    "            \n",
    "        \n",
    "    # Algorithm    \n",
    "        \n",
    "    for m in range(M) :\n",
    "        Im = I[0][m]\n",
    " \n",
    "        Gmm = np.zeros((len(Im), len(Im)))\n",
    "        ligne = 0\n",
    "        for i in Im :\n",
    "            colonne = 0\n",
    "            for j in Im :\n",
    "                Gmm[ligne,colonne] = G[i,j]\n",
    "                colonne += 1\n",
    "            ligne +=1\n",
    "        \n",
    "\n",
    "        if np.linalg.det(Gmm)!=0 :\n",
    "            \n",
    "            nu = np.zeros((2*K+2,1))\n",
    "            w = np.zeros((len(Im),1))\n",
    "            for i in range(len(Im)) :\n",
    "                w[i] = np.vdot(pi,U[Im[i],:]) - eta[Im[i]]\n",
    "            \n",
    "            S = np.linalg.solve(Gmm,w) \n",
    "            \n",
    "            for e in range(len(S)) :\n",
    "                nu[Im[e]] = S[e]\n",
    "            \n",
    "            \n",
    "            if np.any(nu<-10**(-10)) == False  :\n",
    "                A = G.dot(nu)\n",
    "                z = np.zeros((1,2*K+2))\n",
    "                for j in range(2*K+2) :\n",
    "                    z[0][j] = np.vdot(pi,U[j,:]) - eta[j] - A[j]\n",
    "                    \n",
    "                    \n",
    "                if np.all(z<=10**(-10)) == True :\n",
    "                    pi_new = pi\n",
    "                    for i in range(2*K+2) :\n",
    "                        pi_new = pi_new - nu[i]*U[i,:]\n",
    "\n",
    "    piStar = pi_new\n",
    "\n",
    "    # Remove noisy small calculus errors:\n",
    "    piStar = piStar/piStar.sum()\n",
    "    \n",
    "    return piStar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot convergence of the projected subgradient algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_convergence(V_iter):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    V_iter : List\n",
    "        List of value of V at each iteration n.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Plot\n",
    "        Plot of V_pibar.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    figConv = plt.figure(figsize=(8,4))\n",
    "    plt_conv = figConv.add_subplot(1,1,1)\n",
    "    V = V_iter.copy()\n",
    "    V.insert(0,np.min(V))\n",
    "    font = {'weight':'normal','size':16}\n",
    "    plt_conv.plot(V, label='V(pi^(n))')\n",
    "    plt_conv.set_xscale('log')\n",
    "    plt_conv.set_ylim(np.min(V), np.max(V)+0.01)\n",
    "    plt_conv.set_xlim(10**0)\n",
    "    plt_conv.set_xlabel('Interation n', fontdict=font)\n",
    "    plt_conv.set_title('Maximization of V over U', fontdict=font)\n",
    "    plt_conv.grid(True)\n",
    "    plt_conv.grid(which='minor', axis='x', ls='-.')\n",
    "    plt_conv.legend(loc=2, shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Discrete Box-Constrained Minimax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for computing delta_Bayes_discret\n",
    "\n",
    "The function delta_Bayes_discret refers to the Bayes Classifier associated with any priors $\\pi\\in\\mathbb{S}$ as detailed in the equation (19) in the paper [1]:\n",
    "\n",
    "$${\\delta}_{\\pi}^B : X_{i} \\mapsto \\underset{l\\in\\mathcal{\\hat{Y}}}{\\mathrm{arg\\,min}} \\, \\sum_{t\\in\\mathcal{T}}\\sum_{k\\in\\mathcal{Y}} L_{kl} \\, \\pi_k   \\, \\hat{p}_{kt} \\,\\mathbb{1}_{\\{X_{i}=x_{t} \\}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_Bayes_discret(XDTrain, pHat, pi, K, L): \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XDTrain : Array\n",
    "        Features of the training sample after discretization.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array of floats\n",
    "        Real class proportions.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Yhat : Vector\n",
    "        Predicted labels.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Yhat = np.zeros((np.shape(XDTrain)[0], 1))\n",
    "    for i in range(0,np.shape(XDTrain)[0]):\n",
    "        t = int(XDTrain[i])\n",
    "        lambd = np.zeros((1,K))\n",
    "        for l in range(0, K):\n",
    "            for k in range(0, K):\n",
    "                lambd[0,l] = lambd[0,l] + L[k,l] * pi[0,k] * pHat[k,t-1]\n",
    "        lbar = np.argmin(lambd[0,:]) \n",
    "        Yhat[i,0] = lbar + 1\n",
    "    return Yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function predict_DMC\n",
    "\n",
    "The function *predict_DMC* aims to predict the class labels of the test instances using the Discrete Minimax Classifier associated with the priors $\\pi^{\\star}$. This function includes the discretization process of the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_DMC(XTest, K, L, discretization, modele, pHatDMC, piStarDMC):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    XTest : DataFrame\n",
    "        Features of the testing sample.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    discretization : str\n",
    "        The type of discretization used\n",
    "    modele : kmeans or Decision Tree classifier model\n",
    "        The model for the discretization\n",
    "    pHatDMC : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    piStarDMC : Array of floats\n",
    "        Least favorable priors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    YhatTest : Array\n",
    "        Predicted labels.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if discretization == 'kmeans':\n",
    "        XDTest = modele.predict(XTest) + 1\n",
    "        \n",
    "    if discretization == 'DT' :\n",
    "         XDTest = discretisation_DT(XTest, modele)\n",
    "\n",
    "    YhatTest = delta_Bayes_discret(XDTest, pHatDMC, piStarDMC, K, L)\n",
    "    \n",
    "    return YhatTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the function V when dealing with K=2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2D_V_piTrain_piBar(pHat, K, L, T, piTrain, piBar):\n",
    "    '''\n",
    "    param:pHat: probability estimate of observing the features profiles x_t in each class k\n",
    "    param:K: number of classes\n",
    "    param:L: loss function\n",
    "    param:T: optimal number of centroids for the discretization process\n",
    "    param:piTrain: class proportions on the training set\n",
    "    param:piBar: least favorable priors\n",
    "    return:rpiTrain: global risk on the training set\n",
    "    return:rpiBar: V(piBar)     \n",
    "    '''\n",
    "    \n",
    "    xx = np.sort(np.insert(np.linspace(0,1,100), [2, 2], [piTrain[0,0], piBar[0,0]]))\n",
    "    stock_pi = np.zeros((np.shape(xx)[0], K))\n",
    "    stock_r = np.zeros((1, np.shape(xx)[0]))\n",
    "    \n",
    "    i = 0\n",
    "    for x in xx:\n",
    "        stock_pi[i,0] = x\n",
    "        stock_pi[i,1] = 1 - x\n",
    "        i = i + 1\n",
    "    \n",
    "    for i in range(0, np.shape(xx)[0]):\n",
    "        lambd = np.zeros((K,T))\n",
    "        for l in range(0, K):\n",
    "            for t in range(0, T):\n",
    "                for k in range(0, K):\n",
    "                    lambd[l,t] = lambd[l,t] + L[k,l] * stock_pi[i,k] * pHat[k,t]\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:,t])\n",
    "                mu_k = mu_k + L[k,lbar] * pHat[k,t]\n",
    "            R[0,k] = mu_k\n",
    "        stock_r[0,i] = np.dot(R[0], stock_pi[i,:])\n",
    "        \n",
    "        if stock_pi[i,0] == piTrain[0,0]:\n",
    "            RpiTrain = R\n",
    "            rpiTrain = np.dot(piTrain[0], RpiTrain[0])\n",
    "            stock_r_piTrain = np.dot(stock_pi, (RpiTrain).T)\n",
    "        \n",
    "        if stock_pi[i,0] == piBar[0,0]:\n",
    "            RpiBar = R\n",
    "            rpiBar = np.dot(piBar[0],RpiBar[0])\n",
    "            stock_r_piBar = np.dot(stock_pi, (RpiBar).T)\n",
    "    \n",
    "    # Figure\n",
    "    \n",
    "    plt.plot(xx, stock_r[0], color = 'blue', label='Function V')\n",
    "    plt.plot(xx, stock_r_piTrain, color = 'orange', label='Discrete Bayes Classifier')\n",
    "    plt.vlines(x = piTrain[0,0], ymin=0, ymax=rpiTrain, linewidth=2, color='orange', linestyle = \"--\")\n",
    "    plt.plot(xx, stock_r_piBar, color = 'green', label='Discrete Minimax Classifier')\n",
    "    plt.vlines(x = piBar[0,0], ymin = 0, ymax = rpiBar, linewidth = 2, color = 'green', linestyle = \"--\")\n",
    "    \n",
    "    plt.rc('text', usetex=True)\n",
    "    font = {'weight':'normal','size':20}\n",
    "    #plt.title('Discrete empirical Bayes risk as a function of the priors', fontdict=font)\n",
    "    plt.xlabel('Prior $\\pi_1$', fontdict=font)\n",
    "    plt.ylabel('Global risk $r$', fontdict=font)\n",
    "    \n",
    "    plt.text(1,RpiTrain[0,0],r'$\\hat{R}_1\\left(\\delta_{\\hat{\\pi}}^B\\right)$',{'color':'orange','fontsize':20})\n",
    "    plt.text(-0.2,RpiTrain[0,1],r'$\\hat{R}_2\\left(\\delta_{\\hat{\\pi}}^B\\right)$',{'color':'orange','fontsize':20})\n",
    "    plt.text(piTrain[0,0]-0.02,-0.12,r'$\\hat{\\pi}_1$',{'color':'orange','fontsize':20})\n",
    "    \n",
    "    plt.text(1,RpiBar[0,0],r'$\\hat{R}_1\\left(\\delta_{\\bar{\\pi}}^B\\right)$',{'color':'green','fontsize':20})\n",
    "    plt.text(-0.2,RpiBar[0,1],r'$\\hat{R}_2\\left(\\delta_{\\bar{\\pi}}^B\\right)$',{'color':'green','fontsize':20})\n",
    "    plt.text(piBar[0,0]-0.02,-0.12,r'$\\bar{\\pi}_1$',{'color':'green','fontsize':20})\n",
    "    \n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=1, shadow=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return rpiTrain, rpiBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Softmin Discrete Minimax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftminDMC_predict(X, K, L, discretization, modele, pHat, pi, lambd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        Features of the instances.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    discretization : str\n",
    "        The type of discretization used\n",
    "    modele : kmeans or Decision Tree classifier model\n",
    "        The model for the discretization\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    OutputProba : Array\n",
    "        Estimated probabilties of each instance to belong in each class.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if discretization == 'kmeans':\n",
    "        XD = modele.predict(X) + 1\n",
    "        \n",
    "    if discretization == 'DT' :\n",
    "        XD = discretisation_DT(X, modele)\n",
    "\n",
    "    \n",
    "    Yhat = np.zeros((np.shape(XD)[0], 1))\n",
    "    OutputProba = np.zeros((np.shape(XD)[0], K))\n",
    "    \n",
    "    for i in range(0,np.shape(XD)[0]):\n",
    "        t = int(XD[i])\n",
    "        F = np.zeros((1,K))\n",
    "        NUM_SIGM = np.zeros((1,K))\n",
    "        for l in range(0, K):\n",
    "            for k in range(0, K):\n",
    "                F[0,l] = F[0,l] + L[k,l] * pi[0,k] * pHat[k,t-1]\n",
    "            NUM_SIGM[0,l] = np.exp(-lambd * F[0,l])\n",
    "        OutputProba[i,:] = NUM_SIGM/np.sum(NUM_SIGM)\n",
    "        Yhat[i,0] = np.argmax(np.random.multinomial(1,OutputProba[i,:])) + 1\n",
    "    \n",
    "    return Yhat, OutputProba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftminDMC_compute_CondRisks(K, L, pHat, pi, lambd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    R_softminDMC : Array\n",
    "        Values of the class-conditional risks at the point pi.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    T = np.shape(pHat)[1]\n",
    "    F = np.zeros((K,T))\n",
    "    NUM_SIGM = np.zeros((K,T))\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, K):\n",
    "            for j in range(0, K):\n",
    "                F[l,t] = F[l,t] + L[j,l] * pHat[j,t] * pi[0,j]\n",
    "            NUM_SIGM[l,t] = np.exp(-lambd * F[l,t])\n",
    "    DENUM_SIGM = np.sum(NUM_SIGM, axis=0)\n",
    "\n",
    "    R_softminDMC = np.zeros((1, K))\n",
    "    for k in range(0, K):\n",
    "        mu_k = 0\n",
    "        for t in range(0, T):\n",
    "            for l in range(0, K):\n",
    "                sigma_l = NUM_SIGM[l,t]/DENUM_SIGM[t]\n",
    "                mu_k = mu_k + L[k,l] * pHat[k,t] * sigma_l\n",
    "        R_softminDMC[0,k] = mu_k\n",
    "\n",
    "    return R_softminDMC[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normGpi(K, L, pHat, pi, lambd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normGpi : Float\n",
    "        Value of the norm of G(pi).\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    R_softminDMC = SoftminDMC_compute_CondRisks(K, L, pHat, pi, lambd)\n",
    "    V_pi = pi[0].dot(R_softminDMC)\n",
    "    normGpi = np.linalg.norm(R_softminDMC-V_pi)\n",
    "\n",
    "    return normGpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftminDMC_compute_G_root_MonteCarlo(K, L, pHat, lambd, N, Alpha, epsilon):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    N : int\n",
    "        Maximum number of iterations for the imulated annealing algorithm.\n",
    "    Alpha : Array\n",
    "        Parameter for the Dirichlet distribution.\n",
    "    epsilon : Float\n",
    "        Positive threshold allowing to accept piSTAR.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    piSTAR : Array\n",
    "        The priors for which the application G vanishes.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    c = np.sum(np.sum(L,1)*np.sum(L,1))\n",
    "    piSTAR = np.random.dirichlet(Alpha,1)\n",
    "    normGpiStar = compute_normGpi(K, L, pHat, piSTAR, lambd)\n",
    "    pi = np.copy(piSTAR)\n",
    "    normGpi = normGpiStar\n",
    "    \n",
    "    for n in range(0, N):\n",
    "        \n",
    "        if normGpi < normGpiStar:\n",
    "            normGpiStar = normGpi\n",
    "            piSTAR[:] = pi[:]\n",
    "            print('At iteration n =', n)\n",
    "            print('----> normGpiStar =', normGpiStar)\n",
    "            if normGpiStar <= epsilon:\n",
    "                break\n",
    "                \n",
    "        tau = np.random.dirichlet(Alpha,1)\n",
    "        normGtau = compute_normGpi(K, L, pHat, tau, lambd)\n",
    "        pi[:] = tau[:]\n",
    "        normGpi = normGtau\n",
    "            \n",
    "        \n",
    "    print('Final iteration at n =', n)\n",
    "    print('----> normGpiStar =', normGpiStar)\n",
    "        \n",
    "    return piSTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftminDMC_compute_G_root_ascent_Step(K, L, pHat, lambd, N, epsilon):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    N : int\n",
    "        Maximum number of iterations for the imulated annealing algorithm.\n",
    "    epsilon : Float\n",
    "        Positive threshold allowing to accept piSTAR.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    piSTAR : Array\n",
    "        The priors for which the application G vanishes.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #np.random.seed(407)\n",
    "    pi_init = np.ones((1,K))/K\n",
    "    normGpiStar = compute_normGpi(K, L, pHat, pi_init, lambd)\n",
    "    pi = pi_init.copy()\n",
    "    piSTAR = pi_init.copy()\n",
    "\n",
    "\n",
    "    for n in range(1,N):\n",
    "\n",
    "        R_softminDMC = SoftminDMC_compute_CondRisks(K, L, pHat, pi, lambd)\n",
    "        V_pi = pi[0].dot(R_softminDMC)\n",
    "        Gpi = R_softminDMC - V_pi\n",
    "        normGpi = np.linalg.norm(Gpi)\n",
    "\n",
    "        if normGpi < normGpiStar:\n",
    "            normGpiStar = normGpi\n",
    "            piSTAR[:] = pi[:]\n",
    "            if normGpiStar <= epsilon:\n",
    "                break\n",
    "\n",
    "        gamma = 1/n\n",
    "        eta = np.maximum(float(1),normGpi)\n",
    "        w = pi + (gamma/eta)*Gpi   \n",
    "        pi = proj_simplex_Condat(K,w)\n",
    "        \n",
    "    \n",
    "    print('Final iteration at N =', n)\n",
    "    print('----> normGpiStar =', normGpiStar)\n",
    "\n",
    "    return piSTAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
